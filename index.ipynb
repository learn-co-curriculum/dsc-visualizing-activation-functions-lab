{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Activation Functions - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you've built your own CNN and seen how to visualize feature maps, its time to practice loading a pretrained model from file and visualize the learned features systematically. In this lab, you'll expand upon the code from the previous lesson in order to succinctly visualize all the channels from each layer in a CNN.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Load a saved Keras model \n",
    "- Use Keras methods to visualize the activation functions in CNNs \n",
    "\n",
    "## Load a Model  \n",
    "\n",
    "For this lab, load the saved model `'cats_dogs_downsampled_with_augmentation_data.h5'`. This saved file includes both the model architecture and the trained weights. See the `model.save()` method for further details. The model was built in order to help identify cat and dog pictures. Start simply by loading the model and pulling up a summary of the layers. (To load the model use the `keras.models.load_model()` function.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "from keras.models import load_model\n",
    "model = load_model('cats_dogs_downsampled_with_augmentation_data.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an Image\n",
    "\n",
    "Before you plot the learned representations of the convolutional base, let's import an image and display it prior to processing. This will provide a comparison to the transformations formed by the model's feature maps.   \n",
    "\n",
    "Load and display the image `'dog.1100.jpg'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "filename = 'dog.1100.jpg'\n",
    "img = image.load_img(filename, target_size=(150, 150))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Image to a Tensor and Visualize Again\n",
    "\n",
    "Recall that you should always preprocess images into tensors when using deep learning. As such, preprocess this image and then redisplay the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "import numpy as np\n",
    "\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "\n",
    "# Follow the Original Model Preprocessing\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Check tensor shape\n",
    "print(img_tensor.shape)\n",
    "\n",
    "# Preview an image\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Feature Maps\n",
    "\n",
    "Now that you've loaded a model, practice visualizing each of the channels for each of feature maps of the convolutional layers. Recall that this process will take a few steps. First, extract the feature maps, or layer outputs from each of the activation functions in the model. From there, generate models that transform the image from its raw state to these feature maps. You can then take these transformations and visualize each channel for each feature map.  \n",
    "\n",
    "To preview the results of the solution code, take a sneek peak at the *Intermediate_Activations_Visualized.pdf* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "from keras import models\n",
    "import math \n",
    "\n",
    "# Extract model layer outputs\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "\n",
    "# Create a model for displaying the feature maps\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "# Extract Layer Names for Labelling\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "total_features = sum([a.shape[-1] for a in activations])\n",
    "total_features\n",
    "\n",
    "n_cols = 16\n",
    "n_rows = math.ceil(total_features / n_cols)\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "fig , axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols, n_rows*1.5))\n",
    "\n",
    "for layer_n, layer_activation in enumerate(activations):\n",
    "    n_channels = layer_activation.shape[-1]\n",
    "    for ch_idx in range(n_channels):\n",
    "        row = iteration // n_cols\n",
    "        column = iteration % n_cols\n",
    "    \n",
    "        ax = axes[row, column]\n",
    "\n",
    "        channel_image = layer_activation[0,\n",
    "                                         :, :,\n",
    "                                         ch_idx]\n",
    "        # Post-process the feature to make it visually palatable\n",
    "        channel_image -= channel_image.mean()\n",
    "        channel_image /= channel_image.std()\n",
    "        channel_image *= 64\n",
    "        channel_image += 128\n",
    "        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "\n",
    "        ax.imshow(channel_image, aspect='auto', cmap='viridis')\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        \n",
    "        if ch_idx == 0:\n",
    "            ax.set_title(layer_names[layer_n], fontsize=10)\n",
    "        iteration += 1\n",
    "\n",
    "fig.subplots_adjust(hspace=1.25)\n",
    "plt.savefig('Intermediate_Activations_Visualized.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Nice work! In this lab, you practiced loading a model and then visualizing the activation feature maps learned by that model on your data! In the upcoming labs and sections you will build upon the first part of this and see how you can adapt the representations learned by more experienced models to your own applications which may have limited training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
